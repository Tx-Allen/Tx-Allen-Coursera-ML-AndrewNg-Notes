<img width="782" alt="image" src="https://github.com/user-attachments/assets/5a4a8df4-567f-4b44-9c5b-1b82a54d9f42">

## Adam 优化算法:
- **Adam（Adaptive Moment Estimation）** 是一种更加先进的优化算法，它通过自适应地调整每个参数的学习率来加速训练。
- **Adam 可以自动根据梯度的变化调整学习率：**
  - 如果参数更新的方向在几次迭代中保持一致，则 Adam 会增大学习率，让模型更快地向最优解收敛。
  - 如果参数更新方向频繁反转（震荡），则 Adam 会减小学习率，避免来回震荡，进而稳定收敛。
 
<img width="760" alt="image" src="https://github.com/user-attachments/assets/bd9b87ba-8c98-40a3-be03-12a4d42d6bf0">

- **Adam 的优势:**
  - **自适应学习率:** 相较于固定的学习率，Adam 可以根据每个参数的变化调整学习率，使得训练更有效率。
  - **局部学习率: 每个参数（如w1, w2, …, wn）都有不同的学习率**，避免了一刀切的全局学习率。
  - **更快的收敛速度:** 相比于传统的梯度下降算法，Adam 通常能够更快地找到最优解，尤其是在高维和复杂的神经网络中。
 
<img width="787" alt="image" src="https://github.com/user-attachments/assets/f8850efe-6839-492c-bdd1-8df25a924173">
