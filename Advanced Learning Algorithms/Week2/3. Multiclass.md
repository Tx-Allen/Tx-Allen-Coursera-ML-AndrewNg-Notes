<img width="711" alt="image" src="https://github.com/user-attachments/assets/00b9d7b0-ba63-4290-b7b4-dc39f8dce78b">

## **多类(Multiclass)**
  - 定义: 多类分类是分类问题的一种，输出标签不止两个，而是多个可能类别。
  - 示例:
    - 手写数字识别（0-9的10个数字分类）
    - 医疗诊断（多种疾病分类）
    - 制造业缺陷检测（如划痕、变色、碎片等多种缺陷分类）
    - 特征: y 只能处理少量离散类别，而不是连续的数值。相比二元分类，多类分类估计y属于每个类别的概率。

  - 逻辑回归在多类分类中的应用:
    - 给定特征 x，逻辑回归不仅估计 y = 1 的概率，还估计 y 属于每个类别的概率（如 y = 1, y = 2, y = 3 等）。
    - 决策边界: 学习一个决策边界，将输入特征空间划分为多个区域，每个区域对应一个类别。

## SoftMax算法

<img width="902" alt="image" src="https://github.com/user-attachments/assets/f464ef93-0aa3-4ce8-be0a-b21d100ad3ba">
<img width="912" alt="image" src="https://github.com/user-attachments/assets/4bf986c7-9976-48c5-8421-32d015563b31">
<img width="874" alt="image" src="https://github.com/user-attachments/assets/81fc846c-1ece-443d-bdb1-9f7b3fb98f73">

- 使用 ReLU（Rectified Linear Unit） 激活函数的原因在于它可以帮助神经网络更好地学习复杂的模式，尤其是在深层网络中。以下是使用 ReLU 的主要原因：

  - 避免梯度消失问题：在深度神经网络中，像 sigmoid 或 tanh 激活函数可能会导致梯度消失问题，导致网络在训练过程中学习变得非常缓慢。ReLU 函数输出的正值范围为线性（即不压缩），从而可以避免这种情况，并保持较大的梯度，使得训练更加高效。
  - 稀疏激活：ReLU 的输出为非负值，当输入为负数时，ReLU 会输出 0，这意味着网络中的部分神经元不会被激活。这种稀疏性可以使模型更高效，减少计算复杂度，并且能够提升网络的泛化能力。
  - 计算简单：ReLU 的计算非常简单，只需要进行一个比较操作（如果输入小于0，输出0；否则输出原值）。这种简单的计算大大加快了网络的训练速度。
  - 加速收敛：使用 ReLU 激活函数的网络往往能够更快地收敛，即更快找到最优的参数。因为它避免了其他激活函数（如 sigmoid）带来的梯度消失问题，从而能够进行更有效的权重更新。
  - 在图中的网络架构中，前两层使用了 ReLU 激活函数，帮助模型在非线性转换中捕获特征。最后一层是 Softmax 激活函数，专门用于将输出转换为概率分布，以适应多类分类问题。
- **ReLU 适合用于隐藏层的激活函数，而 Softmax 更适合用于多类分类问题的输出层** 。

## Improved implementation of softmax（More accurate）
<img width="650" alt="image" src="https://github.com/user-attachments/assets/12261dc8-3478-4092-b051-89583388d463">
<img width="707" alt="image" src="https://github.com/user-attachments/assets/e4bf6835-4c48-4940-b67b-62c6a742ef30">
<img width="890" alt="image" src="https://github.com/user-attachments/assets/c56d8cd1-f75f-4dfc-8743-e37f9b0ae17b">

- 在 TensorFlow 中，通过使用 BinaryCrossEntropy(from_logits=True)，可以更准确地计算损失函数。from_logits=True 表示 TensorFlow 将直接接受 logits z 作为输入，而不是已经经过 sigmoid 激活的概率值 a。
- 这种方式避免了在计算损失时再次应用 sigmoid 函数，从而减少了数值误差。

<img width="891" alt="image" src="https://github.com/user-attachments/assets/d6f1b647-0ea0-4ca6-ab56-2e4daef514ec">
<img width="921" alt="image" src="https://github.com/user-attachments/assets/ec7a48c9-2b49-4e8d-8dd7-efe44623de52">

- **Sigmoid 和 Softmax 的区别:**
  - **Sigmoid:** 通常用于二元分类问题，输出一个介于 0 到 1 之间的概率。
  - **Softmax:** 用于多类分类问题，输出多个类别的概率，并确保它们之和为 1。
 <img width="774" alt="image" src="https://github.com/user-attachments/assets/7c0caaad-9fd4-46a0-ba84-3e03e457b1d3">

- tf.keras.Input(shape=(400,))
<img width="790" alt="image" src="https://github.com/user-attachments/assets/f7aaf789-d1c2-4c0a-b603-ca5fd306944e">

<img width="921" alt="image" src="https://github.com/user-attachments/assets/c72f3956-ef56-41af-8cc8-9c6ab92c6d6a">

- **from_logits=True 的作用:**
  - 当你使用 from_logits=True 时，TensorFlow 假定输出层的值（logits）还未经过激活函数（即未经过 sigmoid 或 softmax）。在这种情况下，TensorFlow 会自动在计算损失函数时将 logits 转换为相应的概率。
  - 对于二元分类：如果你在最后一层没有使用 sigmoid，直接输出 logits，那么设置 from_logits=True，TensorFlow 会自动将 logits 转换为经过 sigmoid 的概率。
  - 对于多类分类：如果你在最后一层没有使用 softmax，直接输出 logits，设置 from_logits=True，TensorFlow 会自动将 logits 转换为经过 softmax 的概率。
- **代码替换的实现:**
  - **对于 sigmoid 替换：** 使用 BinaryCrossEntropy(from_logits=True)，可以处理二元分类，不需要在最后一层手动加 sigmoid。
    - 如果你在做二元分类任务（比如预测是"猫"还是"狗"），输出层只需要 1 个神经元。
  - **对于 softmax 替换：** 使用 SparseCategoricalCrossEntropy(from_logits=True)，可以处理多类分类，不需要在最后一层手动加 softmax。
    - 如果你在做多类分类任务（比如识别数字0-9），输出层需要 10 个神经元，每个神经元对应一个类别。
- 在深度学习框架（如 TensorFlow 和 Keras）中，units 是用于指定神经网络层中的神经元数量的参数。**它定义了该层中的神经元个数，也决定了该层的输出维度**。
- **units 参数的作用：**
  - 神经元数量: units 决定该层的神经元（或单元）的数量。每个神经元从上一层接收输入并通过激活函数产生输出。
  - 输出维度: 该层的输出维度取决于 units 的值。如果 units=10，这意味着该层有 10 个神经元，输出的维度为 10。

<img width="965" alt="image" src="https://github.com/user-attachments/assets/6959a339-1c09-4549-aa84-cc4737eabdd2">

- **多标签分类的神经网络实现:**
  - 可以把多标签分类看作多个独立的二元分类问题。例如，一个神经网络负责检测是否有汽车，另一个负责检测是否有公共汽车，第三个负责行人检测。
  - 更常见的方法是使用一个神经网络，在最后一层有多个输出节点，每个输出节点对应一个标签，使用 sigmoid 激活函数 来分别判断每个标签的状态。这些输出值可以是 0 或 1，表示是否有对应的标签。
